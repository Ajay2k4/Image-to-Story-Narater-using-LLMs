import openai

# Replace 'your_actual_api_key_here' with your real API key to test directly
openai.api_key = "1a9FVQMs6oYNn2uTFmZUTws5tdLD1K6vKWZYTQc8FatQouuQpteYJQQJ99AJACYeBjFXJ3w3AAABACOGMSJx"

try:
    response = openai.Completion.create(
        model="text-davinci-003",
        prompt="Hello, world!",
        max_tokens=5
    )
    print("API key is valid, response received.")
except openai.error.AuthenticationError as e:
    print(f"Authentication error: {e}")





    '''
import requests
import json

def generate_story_from_text(api_key, generated_text, model_name="gpt-2", max_length=50, temperature=0.7):
    """
    Generate a story from the text generated by the first function using the Hugging Face Inference API.

    Parameters:
    - api_key (str): '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'
    - generated_text (str):  (generate_text_from_image).
    - model_name (str): 'gpt-2'
    - max_length (int): 50
    - temperature (float): 0.7

    Returns:
    - str: The generated story or an error message.
    """
    API_URL = f"https://api-inference.huggingface.co/models/{model_name}"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Prepare the input data
    data = {
        "inputs": generated_text,
        "parameters": {
            "max_length": max_length,
            "temperature": temperature
        }
    }

    # Make the POST request to Hugging Face's API
    response = requests.post(API_URL, headers=headers, json=data)

    if response.status_code == 200:
        result = response.json()
        # Check if the response contains the generated story
        if isinstance(result, list) and "generated_text" in result[0]:
            return result[0]["generated_text"]
        else:
            return "Unexpected response format from the API."
    else:
        return f"Error: {response.status_code}, {response.text}"

# Example usage
generated_text = generate_text_from_image("download.jpg")  # Get generated text from the image
api_key = "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"  # Your Hugging Face API key
story = generate_story_from_text(api_key, generated_text)
print(story)





# Set the endpoint and API key for Gemini or any other Google API
api_key = 'AIzaSyAuTKyvt8U9W9ebnGCN9il01MDCCZFnQiM'  # Replace with your API key
endpoint = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyAuTKyvt8U9W9ebnGCN9il01MDCCZFnQiM" # Replace with actual Gemini endpoint

def generate_story_from_text(prompt: str) -> str:
    """
    Function to generate a story from text using Gemini API directly via API key.
    
    Args:
        prompt (str): The text prompt to generate a story.
    
    Returns:
        str: The generated story or an error message.
    """
    
    # Define the headers with the API key for authentication
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }
    

    # Define the data payload for the request (adjust based on Gemini's API requirements)
    data = {
        "input": prompt,  # The text prompt you want to generate a story from
        "temperature": 0.7,  # You can adjust the temperature for creativity
        "max_tokens": 200  # Max length for the generated response (adjust as needed)
    }    


    try:
        # Make the POST request to the Gemini API
        response = requests.post(endpoint, headers=headers, json=data)
        
        # Check if the response is successful
        if response.status_code == 200:
            response_data = response.json()
            generated_story = response_data.get("story", "")  # Adjust based on actual API response format
            return generated_story
        else:
            return f"Error: {response.status_code}, {response.text}"

    except Exception as e:
        return f"An error occurred: {str(e)}"

# # Example usage
# prompt_text = "Once upon a time in a faraway land, there was a mysterious forest."
# story = story_from_text(prompt_text)
# print("Generated Story:")
# print(story)




def generate_speech_from_text(message: str) -> None:
    """
    Uses ESPnet text-to-speech model from HuggingFace to generate audio.
    :param message: Short story generated by the GPT model.
    """
    API_URL: str = "https://api-inference.huggingface.co/models/espnet/kan-bayashi_ljspeech_vits"
    headers: dict = {"Authorization": f"Bearer {HUGGINGFACE_API_TOKEN}"}
    payloads: dict = {"inputs": message}

    # Make the POST request
    response = requests.post(API_URL, headers=headers, json=payloads)
    
    # Check if the response is valid
    if response.status_code == 200:
        # Save the audio content as a file with a specified format
        with open("generated_audio.wav", "wb") as file:
            file.write(response.content)
    else:
        print(f"Error: {response.status_code} - {response.text}")



import shutil
import os
import streamlit as st

def save_audio(audio_file_path, save_location):
    """
    This function takes the path of the generated audio file and the desired 
    save location, and moves the audio file to that location.
    
    Parameters:
    - audio_file_path (str): Path of the generated audio file
    - save_location (str): Directory path where the audio file should be stored
    
    Returns:
    - str: The final path of the saved audio file
    """
    try:
        # Ensure the save location exists, create if not
        if not os.path.exists(save_location):
            os.makedirs(save_location)
        
        # Extract the audio file name
        audio_file_name = os.path.basename(audio_file_path)
        
        # Construct the destination path
        destination = os.path.join(save_location, audio_file_name)
        
        # Move the audio file to the destination
        shutil.move(audio_file_path, destination)
        
        # Optionally, you can also copy the file instead of moving it
        # shutil.copy(audio_file_path, destination)
        
        st.success(f"Audio saved successfully at: {destination}")
        
        # Return the final saved file path
        return destination
    
    except Exception as e:
        st.error(f"Error while saving the audio file: {e}")
        return None

# Example usage
audio_file_path = "generated_audio.wav"  # Path to your generated audio file
save_location = 'C:\Users\ajayss\genaicp3\Image-to-Story\genrated_audio'  # Change this to your desired directory

saved_audio_path = save_audio(audio_file_path, save_location)

# If you want to play the saved audio in Streamlit
if saved_audio_path:
    st.audio(saved_audio_path, format="audio/wav")

    



def generate_speech_from_text(message: str) -> Any:
    # """
    # A function using the ESPnet text to speech model from HuggingFace
    # :param message: short story generated by the GPT model
    # :return: generated audio from the short story
    # """
    API_URL: str = "https://api-inference.huggingface.co/models/espnet/kan-bayashi_ljspeech_vits"
    headers: dict[str, str] = {"Authorization": f"Bearer {HUGGINGFACE_API_TOKEN}"}
    payloads: dict[str, str] = {
        "inputs": message
    }

    response: Any = requests.post(API_URL, headers=headers, json=payloads)
    with open("generated_audio.flac", "wb") as file:
        file.write(response.content)
    



'''


